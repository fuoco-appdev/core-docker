services:
  ollama:
    volumes:
      - ollama:/root/.ollama
    container_name: ollama
    pull_policy: always
    tty: true
    restart: unless-stopped
    image: ollama/ollama:${OLLAMA_DOCKER_TAG-latest}
    # GPU support
    deploy:
      resources:
        reservations:
          devices:
            - driver: ${OLLAMA_GPU_DRIVER-nvidia}
              count: ${OLLAMA_GPU_COUNT-1}
              capabilities:
                - gpu

  open-webui:
    build:
      context: ../open-webui
      args:
        OLLAMA_BASE_URL: '/ollama'
      dockerfile: Dockerfile
    image: ghcr.io/open-webui/open-webui:${WEBUI_DOCKER_TAG-main}
    container_name: open-webui
    volumes:
      - open-webui:/app/backend/data
    depends_on:
      - ollama
      - standalone
    ports:
      - 127.0.0.1:${OPEN_WEBUI_PORT-3002}:8080
    environment:
      - 'OLLAMA_BASE_URL=http://ollama:11434'
      - 'WEBUI_SECRET_KEY='
      - 'WEBUI_URL=http://127.0.0.1:${OPEN_WEBUI_PORT-3002}'
      - 'VECTOR_DB=milvus'
      - 'MILVUS_URI=${MILVUS_URI}'
    extra_hosts:
      - host.docker.internal:host-gateway
    restart: unless-stopped

  etcd:
    container_name: milvus-etcd
    image: quay.io/coreos/etcd:v3.5.5
    environment:
      - ETCD_AUTO_COMPACTION_MODE=revision
      - ETCD_AUTO_COMPACTION_RETENTION=1000
      - ETCD_QUOTA_BACKEND_BYTES=4294967296
      - ETCD_SNAPSHOT_COUNT=50000
    volumes:
      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/etcd:/etcd
    command: etcd -advertise-client-urls=http://127.0.0.1:2379 -listen-client-urls http://0.0.0.0:2379 --data-dir /etcd
    healthcheck:
      test: [ 'CMD', 'etcdctl', 'endpoint', 'health' ]
      interval: 30s
      timeout: 20s
      retries: 3

  standalone:
    container_name: milvus-standalone
    image: milvusdb/milvus:latest-gpu
    command: [ 'milvus', 'run', 'standalone' ]
    security_opt:
      - seccomp:unconfined
    environment:
      ETCD_ENDPOINTS: etcd:2379
      MINIO_ADDRESS: ${MINIO_ADDRESS}
    volumes:
      - ${DOCKER_VOLUME_DIRECTORY:-.}/volumes/milvus:/var/lib/milvus
      - ./volumes/milvus/configs/milvus.yaml:/milvus/configs/milvus.yaml
    ports:
      - '19530:19530'
      - '9091:9091'
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [ 'gpu' ]
              device_ids: [ '${NVIDIA_GPU_DEVICE_ID:-0}' ]
    depends_on:
      - 'etcd'

  openedai-speech-server:
    image: ghcr.io/matatonic/openedai-speech
    environment:
      TTS_HOME: voices
      HF_HOME: voices
      PRELOAD_MODEL: xtts
      EXTRA_ARGS: --log-level DEBUG --unload-timer 300 --port 8002
      USE_ROCM: 1
    ports:
      - "8002:8002"
    volumes:
      - ../openedai-speech/voices:/app/voices
      - ../openedai-speech/config:/app/config
    # To install as a service
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              #device_ids: ['0', '1'] # Select a gpu, or
              count: all
              capabilities: [ gpu ]

networks:
  default:
    name: milvus

volumes:
  ollama: {}
  open-webui: {}
  etcd: {}
  standalone: {}
